# ============================================================
# LLM Configuration
# ============================================================

# Option 1: Use a custom OpenAI-compatible endpoint (local LLM)
# For local LLMs like Ollama, LM Studio, vLLM, etc.
OPENAI_ENDPOINT=http://host.docker.internal:11434/v1
LLM_MODEL=llama3.2
# OPENAI_API_KEY is optional for local endpoints (some don't require it)
# OPENAI_API_KEY=not-needed

# Option 2: Use OpenAI API
# OPENAI_API_KEY=sk-...
# LLM_MODEL=gpt-4

# Option 3: Use Anthropic API
# ANTHROPIC_API_KEY=sk-ant-...
# LLM_MODEL=claude-3-5-sonnet-20241022

# ============================================================
# MCP Server Configuration
# ============================================================

# MCP Server URL (optional, defaults to http://server:80/mcp)
# MCP_URL=http://localhost:8080/mcp

# ============================================================
# Debug Configuration
# ============================================================

# Debug mode (optional)
# DEBUG=true
