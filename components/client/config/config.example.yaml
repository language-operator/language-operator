# Langop MCP Chat Client Configuration
# Copy this file to config.yaml and customize for your setup

# LLM Configuration
llm:
  # Provider: openai, anthropic, or custom endpoint
  provider: openai_compatible

  # Model to use
  model: llama3.2

  # For custom OpenAI-compatible endpoints (Ollama, LM Studio, vLLM, etc.)
  endpoint: http://host.docker.internal:11434/v1

  # API key (optional for local endpoints, required for OpenAI/Anthropic)
  # api_key: sk-...

  # Alternative configurations (comment/uncomment as needed):

  # For OpenAI:
  # provider: openai
  # model: gpt-4
  # api_key: sk-...

  # For Anthropic:
  # provider: anthropic
  # model: claude-3-5-sonnet-20241022
  # api_key: sk-ant-...

# MCP Servers Configuration
mcp_servers:
  # Main based server
  - name: based-server
    url: http://server:80/mcp
    transport: streamable
    enabled: true
    description: "Main based MCP server with core tools"

  # Documentation tools server
  - name: doc-tools
    url: http://doc:80/mcp
    transport: streamable
    enabled: true
    description: "Documentation generation and man page tools"

  # Email tools server
  - name: email-tools
    url: http://email:80/mcp
    transport: streamable
    enabled: true
    description: "Email sending and management tools"

  # SMS tools server
  - name: sms-tools
    url: http://sms:80/mcp
    transport: streamable
    enabled: true
    description: "SMS sending tools (Twilio/Vonage)"

  # Web tools server
  - name: web-tools
    url: http://web:80/mcp
    transport: streamable
    enabled: true
    description: "Web scraping and HTTP request tools"

  # Example: External MCP server
  # - name: external-server
  #   url: http://192.168.1.100:8080/mcp
  #   transport: streamable
  #   enabled: false
  #   description: "External MCP server on local network"

# Debug mode
debug: false
